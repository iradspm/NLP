{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63c3f31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils import data\n",
    "import torch.optim as optim\n",
    "import nltk\n",
    "from pytorch_pretrained_bert import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0dd5814e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8384ed67",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_file = open(\"corpus\", \"r\")\n",
    "all_lines = training_file.readlines()\n",
    "training_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a493594c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_words():\n",
    "    for words in all_lines:\n",
    "        split_words=words.split(' ')\n",
    "    return split_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ab32af8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Tene/JJ',\n",
       " 'yĩla/RB',\n",
       " 'Ĩsilaeli/NP',\n",
       " \"yatongoew'e/VB\",\n",
       " 'nĩ/PRE',\n",
       " 'Asili/NNS',\n",
       " ',/COMMA',\n",
       " 'nĩkweethĩiwe/VB',\n",
       " 'na/CONJ',\n",
       " 'yũa/NN',\n",
       " 'nthĩ/NN',\n",
       " 'ĩsu/JJ',\n",
       " './.',\n",
       " 'Kwoou/JJ',\n",
       " 'mũndũ/NN',\n",
       " 'ũmwe/JJ',\n",
       " 'kuma/PRE',\n",
       " 'Mbetheleemu/NP',\n",
       " 'nthĩ/NN',\n",
       " 'ya/PRE',\n",
       " 'Yuta/NP',\n",
       " 'nĩwaendie/VB',\n",
       " 'e/RB',\n",
       " 'na/CONJ',\n",
       " 'mũka/NN',\n",
       " 'na/CONJ',\n",
       " 'ana/NNS',\n",
       " 'make/PP$',\n",
       " 'elĩ/NUM',\n",
       " 'kwĩkala/VB',\n",
       " 'kwa/RB',\n",
       " 'kavinda/NN',\n",
       " 'ũeninĩ/JJ',\n",
       " 'nthĩ/NN',\n",
       " 'ya/PRE',\n",
       " 'Moavi/NP',\n",
       " './.',\n",
       " 'Mũndũ/NN',\n",
       " 'ũsu/JJ',\n",
       " 'eetawa/VB',\n",
       " 'Elimeleki/NP',\n",
       " ',/COMMA',\n",
       " 'na/CONJ',\n",
       " 'mũka/NN',\n",
       " 'eetawa/VB',\n",
       " 'Naũmi/NP',\n",
       " './.',\n",
       " 'Ana/NNS',\n",
       " 'make/PP$',\n",
       " 'ũmwe/NUM',\n",
       " 'eetawa/VB',\n",
       " 'Maloni/NP',\n",
       " 'na/CONJ',\n",
       " 'ũla/DET',\n",
       " 'ũngĩ/JJ',\n",
       " 'eetawa/VB',\n",
       " 'Kilioni/NP',\n",
       " './.',\n",
       " 'Andũ/NNS',\n",
       " 'asu/JJ',\n",
       " 'maĩ/VB',\n",
       " 'ma/PP$',\n",
       " 'mũsyĩ/NN',\n",
       " 'wa/PRE',\n",
       " 'Aevilathi/NPS',\n",
       " 'ala/JJ',\n",
       " 'matwĩe/VB',\n",
       " 'Mbetheleemu/NP',\n",
       " 'nthĩ/NN',\n",
       " 'ya/PRE',\n",
       " 'Yuta/NP',\n",
       " './.',\n",
       " 'Nĩmaendie/VB',\n",
       " 'Moavi/NP',\n",
       " 'matũa/VB',\n",
       " \"kw'o/RB\",\n",
       " './.',\n",
       " 'Elimeleki/NP',\n",
       " 'mũũme/NN',\n",
       " 'wa/PRE',\n",
       " 'Naũmi/NP',\n",
       " 'nĩwakwie/VB',\n",
       " ',/COMMA',\n",
       " 'na/CONJ',\n",
       " 'amũtia/VB',\n",
       " 'e/RB',\n",
       " 'na/CONJ',\n",
       " 'ana/NNS',\n",
       " 'make/PP$',\n",
       " 'elĩ/NUM',\n",
       " './.',\n",
       " 'Ana/NNS',\n",
       " 'asu/JJ',\n",
       " 'nĩmatwaie/VB',\n",
       " 'aka/NNS',\n",
       " 'Amoavi/NPS',\n",
       " './.',\n",
       " 'Ũmwe/NN',\n",
       " 'eetawa/VB',\n",
       " 'Oliva/NP',\n",
       " ',/COMMA',\n",
       " 'na/CONJ',\n",
       " 'ũla/DET',\n",
       " 'ũngĩ/JJ',\n",
       " 'Lũthi/NP',\n",
       " './.',\n",
       " 'Ĩtina/RB',\n",
       " 'wa/CONJ',\n",
       " 'kwĩkala/VB',\n",
       " 'nthĩ/NN',\n",
       " 'ĩsu/JJ',\n",
       " 'vandũ/JJ',\n",
       " 'va/CONJ',\n",
       " 'myaka/NN',\n",
       " 'vakuvĩ/RB',\n",
       " 'ĩkũmi/NUM',\n",
       " ',/COMMA',\n",
       " 'Maloni/NP',\n",
       " 'na/CONJ',\n",
       " 'Kilioni/NP',\n",
       " 'nĩmakwie/VB',\n",
       " 'o/RB',\n",
       " 'namo/PP$',\n",
       " ',/COMMA',\n",
       " 'Naũmi/NP',\n",
       " 'atiwa/VB',\n",
       " 'e/RB',\n",
       " 'weka/RB',\n",
       " ',/COMMA',\n",
       " 'ate/PRE',\n",
       " 'syana/NNS',\n",
       " 'kana/PRE',\n",
       " 'mũũme/NN',\n",
       " './.\\n']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged=list(split_words())\n",
    "tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4860f084",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "134"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#categorize and tag words using str2tupple\n",
    "tagged_sents=[nltk.tag.str2tuple(t) for t in tagged]\n",
    "len(tagged_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0cc2fe8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Tene', 'JJ')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged_sents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3246c36d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PP$',\n",
       " 'NPS',\n",
       " 'NN',\n",
       " 'PRE',\n",
       " '.',\n",
       " 'NP',\n",
       " 'RB',\n",
       " 'VB',\n",
       " 'CONJ',\n",
       " 'NUM',\n",
       " 'NNS',\n",
       " 'COMMA',\n",
       " '.\\n',\n",
       " 'DET',\n",
       " 'JJ']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tags = list(set(word_pos[1] for sent in tagged_tokens for word_pos in sent))\n",
    "tags=list(set([pair[1] for pair in tagged_sents]))\n",
    "tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "251ec872",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PP$,NPS,NN,PRE,.,NP,RB,VB,CONJ,NUM,NNS,COMMA,.\\n,DET,JJ'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\",\".join(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8ce851ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# By convention, the 0'th slot is reserved for padding.\n",
    "tags = [\"<pad>\"] + tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e326a30e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<pad>',\n",
       " 'PP$',\n",
       " 'NPS',\n",
       " 'NN',\n",
       " 'PRE',\n",
       " '.',\n",
       " 'NP',\n",
       " 'RB',\n",
       " 'VB',\n",
       " 'CONJ',\n",
       " 'NUM',\n",
       " 'NNS',\n",
       " 'COMMA',\n",
       " '.\\n',\n",
       " 'DET',\n",
       " 'JJ']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6cf908d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tag2idx = {tag:idx for idx, tag in enumerate(tags)}\n",
    "idx2tag = {idx:tag for idx, tag in enumerate(tags)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d646b62b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<pad>': 0,\n",
       " 'PP$': 1,\n",
       " 'NPS': 2,\n",
       " 'NN': 3,\n",
       " 'PRE': 4,\n",
       " '.': 5,\n",
       " 'NP': 6,\n",
       " 'RB': 7,\n",
       " 'VB': 8,\n",
       " 'CONJ': 9,\n",
       " 'NUM': 10,\n",
       " 'NNS': 11,\n",
       " 'COMMA': 12,\n",
       " '.\\n': 13,\n",
       " 'DET': 14,\n",
       " 'JJ': 15}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e1ed8e2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '<pad>',\n",
       " 1: 'PP$',\n",
       " 2: 'NPS',\n",
       " 3: 'NN',\n",
       " 4: 'PRE',\n",
       " 5: '.',\n",
       " 6: 'NP',\n",
       " 7: 'RB',\n",
       " 8: 'VB',\n",
       " 9: 'CONJ',\n",
       " 10: 'NUM',\n",
       " 11: 'NNS',\n",
       " 12: 'COMMA',\n",
       " 13: '.\\n',\n",
       " 14: 'DET',\n",
       " 15: 'JJ'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx2tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5137b4a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120, 14)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's split the data into train and test (or eval)\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_data, test_data = train_test_split(tagged_sents, test_size=.1)\n",
    "len(train_data), len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "80fe2f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5c43af62",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased', do_lower_case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ed5c67e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sents,tags_li=[],[]\n",
    "# words = [word_pos[0] for word_pos in tagged_sents]\n",
    "# tags = [word_pos[1] for word_pos in tagged_sents]\n",
    "# sents.append([\"[CLS]\"] + words + [\"[SEP]\"])\n",
    "# tags_li.append([\"<pad>\"] + tags + [\"<pad>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "33758062",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PosDataset(data.Dataset):\n",
    "    def __init__(self, tagged_sents):\n",
    "        sents, tags_li = [], [] # list of lists\n",
    "        words = [word_pos[0] for word_pos in tagged_sents]\n",
    "        tags = [word_pos[1] for word_pos in tagged_sents]\n",
    "        sents.append([\"[CLS]\"] + words + [\"[SEP]\"])\n",
    "        tags_li.append([\"<pad>\"] + tags + [\"<pad>\"])\n",
    "        self.sents, self.tags_li = sents, tags_li\n",
    "#         for sent in tagged_sents:\n",
    "#             words = [word_pos[0] for word_pos in sent]\n",
    "#             tags = [word_pos[1] for word_pos in sent]\n",
    "#             sents.append([\"[CLS]\"] + words + [\"[SEP]\"])\n",
    "#             tags_li.append([\"<pad>\"] + tags + [\"<pad>\"])\n",
    "#         self.sents, self.tags_li = sents, tags_li\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sents)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        words, tags = self.sents[idx], self.tags_li[idx] # words, tags: string list\n",
    "\n",
    "        # We give credits only to the first piece.\n",
    "        x, y = [], [] # list of ids\n",
    "        is_heads = [] # list. 1: the token is the first piece of a word\n",
    "        for w, t in zip(words, tags):\n",
    "            tokens = tokenizer.tokenize(w) if w not in (\"[CLS]\", \"[SEP]\") else [w]\n",
    "            xx = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "            is_head = [1] + [0]*(len(tokens) - 1)\n",
    "\n",
    "            t = [t] + [\"<pad>\"] * (len(tokens) - 1)  # <PAD>: no decision\n",
    "            yy = [tag2idx[each] for each in t]  # (T,)\n",
    "\n",
    "            x.extend(xx)\n",
    "            is_heads.extend(is_head)\n",
    "            y.extend(yy)\n",
    "        \n",
    "        assert len(x)==len(y)==len(is_heads), \"len(x)={}, len(y)={}, len(is_heads)={}\".format(len(x), len(y), len(is_heads))\n",
    "\n",
    "        # seqlen\n",
    "        seqlen = len(y)\n",
    "\n",
    "        # to string\n",
    "        words = \" \".join(words)\n",
    "        tags = \" \".join(tags)\n",
    "        return words, x, is_heads, tags, y, seqlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a1cfe709",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad(batch):\n",
    "    '''Pads to the longest sample'''\n",
    "    f = lambda x: [sample[x] for sample in batch]\n",
    "    words = f(0)\n",
    "    is_heads = f(2)\n",
    "    tags = f(3)\n",
    "    seqlens = f(-1)\n",
    "    maxlen = np.array(seqlens).max()\n",
    "\n",
    "    f = lambda x, seqlen: [sample[x] + [0] * (seqlen - len(sample[x])) for sample in batch] # 0: <pad>\n",
    "    x = f(1, maxlen)\n",
    "    y = f(-2, maxlen)\n",
    "\n",
    "\n",
    "    f = torch.LongTensor\n",
    "\n",
    "    return words, f(x), is_heads, tags, f(y), seqlens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "336eaf1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_pretrained_bert import BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "85411efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, vocab_size=None):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-cased')\n",
    "\n",
    "        self.fc = nn.Linear(768, vocab_size)\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        '''\n",
    "        x: (N, T). int64\n",
    "        y: (N, T). int64\n",
    "        '''\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        if self.training:\n",
    "            self.bert.train()\n",
    "            encoded_layers, _ = self.bert(x)\n",
    "            enc = encoded_layers[-1]\n",
    "        else:\n",
    "            self.bert.eval()\n",
    "            with torch.no_grad():\n",
    "                encoded_layers, _ = self.bert(x)\n",
    "                enc = encoded_layers[-1]\n",
    "        \n",
    "        logits = self.fc(enc)\n",
    "        y_hat = logits.argmax(-1)\n",
    "        return logits, y, y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "295b69f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    model.train()\n",
    "    for i, batch in enumerate(iterator):\n",
    "        words, x, is_heads, tags, y, seqlens = batch\n",
    "        _y = y # for monitoring\n",
    "        optimizer.zero_grad()\n",
    "        logits, y, _ = model(x, y) # logits: (N, T, VOCAB), y: (N, T)\n",
    "\n",
    "        logits = logits.view(-1, logits.shape[-1]) # (N*T, VOCAB)\n",
    "        y = y.view(-1)  # (N*T,)\n",
    "\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        if i%10==0: # monitoring\n",
    "            print(\"step: {}, loss: {}\".format(i, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a23c1e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model, iterator):\n",
    "    model.eval()\n",
    "\n",
    "    Words, Is_heads, Tags, Y, Y_hat = [], [], [], [], []\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(iterator):\n",
    "            words, x, is_heads, tags, y, seqlens = batch\n",
    "\n",
    "            _, _, y_hat = model(x, y)  # y_hat: (N, T)\n",
    "\n",
    "            Words.extend(words)\n",
    "            Is_heads.extend(is_heads)\n",
    "            Tags.extend(tags)\n",
    "            Y.extend(y.numpy().tolist())\n",
    "            Y_hat.extend(y_hat.cpu().numpy().tolist())\n",
    "\n",
    "    ## gets results and save\n",
    "    with open(\"result\", 'w') as fout:\n",
    "        for words, is_heads, tags, y_hat in zip(Words, Is_heads, Tags, Y_hat):\n",
    "            y_hat = [hat for head, hat in zip(is_heads, y_hat) if head == 1]\n",
    "            preds = [idx2tag[hat] for hat in y_hat]\n",
    "            assert len(preds)==len(words.split())==len(tags.split())\n",
    "            for w, t, p in zip(words.split()[1:-1], tags.split()[1:-1], preds[1:-1]):\n",
    "                fout.write(\"{} {} {}\\n\".format(w, t, p))\n",
    "            fout.write(\"\\n\")\n",
    "            \n",
    "    ## calc metric\n",
    "    y_true =  np.array([tag2idx[line.split()[1]] for line in open('result', 'r').read().splitlines() if len(line) > 0])\n",
    "    y_pred =  np.array([tag2idx[line.split()[2]] for line in open('result', 'r').read().splitlines() if len(line) > 0])\n",
    "\n",
    "    acc = (y_true==y_pred).astype(np.int32).sum() / len(y_true)\n",
    "\n",
    "    print(\"acc=%.2f\"%acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "cefbf170",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net(vocab_size=len(tag2idx))\n",
    "model.to(device)\n",
    "model = nn.DataParallel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "bd9b66cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = PosDataset(train_data)\n",
    "eval_dataset = PosDataset(test_data)\n",
    "\n",
    "train_iter = data.DataLoader(dataset=train_dataset,\n",
    "                             batch_size=8,\n",
    "                             shuffle=True,\n",
    "                             num_workers=1,\n",
    "                             collate_fn=pad)\n",
    "test_iter = data.DataLoader(dataset=eval_dataset,\n",
    "                             batch_size=8,\n",
    "                             shuffle=False,\n",
    "                             num_workers=1,\n",
    "                             collate_fn=pad)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.0001)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b05d2fdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, loss: 2.8034958839416504\n"
     ]
    }
   ],
   "source": [
    "train(model, train_iter, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a10a4801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc=0.43\n"
     ]
    }
   ],
   "source": [
    "eval(model, test_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5f4a4ed0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nĩkweethĩiwe VB VB',\n",
       " 'elĩ NUM NN',\n",
       " ', COMMA COMMA',\n",
       " 'make PP$ RB',\n",
       " 'ĩkũmi NUM NP',\n",
       " 'mũndũ NN NN',\n",
       " 'Moavi NP NP',\n",
       " 'ũla DET JJ',\n",
       " 'Mbetheleemu NP NP',\n",
       " 'na CONJ VB',\n",
       " 'nthĩ NN NN',\n",
       " 'nĩ PRE VB',\n",
       " 'ana NNS NP',\n",
       " 'Kwoou JJ VB',\n",
       " '']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open('result', 'r').read().splitlines()[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827e5cd5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
